{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "Они уже обработанные и токенизированные. Процесс можно посмотреть в тетрадке 1.1 Processing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/kseniadudiy/Documents/GitHub/DL-for-NLP/HW#1/data/processed_corpus.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "второй UNK год окончательно разочаровать решить податься альфабанк\n",
      "вернуть денежный средство лицевой счёт либо зачесть счёт погашение кредит\n",
      "притом ситуация решиться участие течение сутки заявить\n",
      "мой ##число летний жизнь это самый неповоротливый работник банк который видеть\n",
      "везде написать вклад принимать очередь это\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:5]:\n",
    "    print(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вернуть денежный средство лицевой счёт либо зачесть счёт погашение кредит'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализуйте разделение предложения на примеры методом CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_split(tokens, window, pad_token='PAD'):\n",
    "    \n",
    "    splits = []\n",
    "    vocab = set(tokens)\n",
    "\n",
    "    \n",
    "    for i in range(0, len(tokens)):\n",
    "        left_context = []\n",
    "        right_context = []\n",
    "        for c in reversed(range(1, window+1)):\n",
    "            if i < window:\n",
    "                if i == 0:\n",
    "                    left_context.append(pad_token)\n",
    "                elif tokens.index(tokens[i - c]) >= window:\n",
    "                    left_context.append(pad_token)\n",
    "                else:\n",
    "                    left_context.append(tokens[i - c])\n",
    "            else:\n",
    "                left_context.append(tokens[i - c])\n",
    "        for c in range(1, window+1):\n",
    "            try:\n",
    "                right_context.append(tokens[i + c])\n",
    "            except IndexError:\n",
    "                right_context.append(pad_token)\n",
    "        target = tokens[i]\n",
    "        splits.append((left_context, target, right_context))\n",
    "        \n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['PAD', 'PAD'], 'вернуть', ['денежный', 'средство']), (['PAD', 'вернуть'], 'денежный', ['средство', 'лицевой']), (['вернуть', 'денежный'], 'средство', ['лицевой', 'счёт']), (['денежный', 'средство'], 'лицевой', ['счёт', 'либо']), (['средство', 'лицевой'], 'счёт', ['либо', 'зачесть']), (['лицевой', 'счёт'], 'либо', ['зачесть', 'счёт']), (['счёт', 'либо'], 'зачесть', ['счёт', 'погашение']), (['либо', 'зачесть'], 'счёт', ['погашение', 'кредит']), (['зачесть', 'счёт'], 'погашение', ['кредит', 'PAD']), (['счёт', 'погашение'], 'кредит', ['PAD', 'PAD'])]\n"
     ]
    }
   ],
   "source": [
    "splits = cbow_split(sample_text, window=2)\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Левый контекст: ['PAD', 'PAD']\n",
      "Центральное слово: вернуть\n",
      "Правый контекст: ['денежный', 'средство']\n",
      "\n",
      "Левый контекст: ['PAD', 'вернуть']\n",
      "Центральное слово: денежный\n",
      "Правый контекст: ['средство', 'лицевой']\n",
      "\n",
      "Левый контекст: ['вернуть', 'денежный']\n",
      "Центральное слово: средство\n",
      "Правый контекст: ['лицевой', 'счёт']\n",
      "\n",
      "Левый контекст: ['денежный', 'средство']\n",
      "Центральное слово: лицевой\n",
      "Правый контекст: ['счёт', 'либо']\n",
      "\n",
      "Левый контекст: ['средство', 'лицевой']\n",
      "Центральное слово: счёт\n",
      "Правый контекст: ['либо', 'зачесть']\n",
      "\n",
      "Левый контекст: ['лицевой', 'счёт']\n",
      "Центральное слово: либо\n",
      "Правый контекст: ['зачесть', 'счёт']\n",
      "\n",
      "Левый контекст: ['счёт', 'либо']\n",
      "Центральное слово: зачесть\n",
      "Правый контекст: ['счёт', 'погашение']\n",
      "\n",
      "Левый контекст: ['либо', 'зачесть']\n",
      "Центральное слово: счёт\n",
      "Правый контекст: ['погашение', 'кредит']\n",
      "\n",
      "Левый контекст: ['зачесть', 'счёт']\n",
      "Центральное слово: погашение\n",
      "Правый контекст: ['кредит', 'PAD']\n",
      "\n",
      "Левый контекст: ['счёт', 'погашение']\n",
      "Центральное слово: кредит\n",
      "Правый контекст: ['PAD', 'PAD']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in splits:\n",
    "    print('Левый контекст:', sample[0])\n",
    "    print('Центральное слово:', sample[1])\n",
    "    print('Правый контекст:', sample[2], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['PAD', 'PAD'], 'вернуть', ['денежный', 'средство']),\n",
       " (['PAD', 'вернуть'], 'денежный', ['средство', 'лицевой']),\n",
       " (['вернуть', 'денежный'], 'средство', ['лицевой', 'счёт']),\n",
       " (['денежный', 'средство'], 'лицевой', ['счёт', 'либо']),\n",
       " (['средство', 'лицевой'], 'счёт', ['либо', 'зачесть']),\n",
       " (['лицевой', 'счёт'], 'либо', ['зачесть', 'счёт']),\n",
       " (['счёт', 'либо'], 'зачесть', ['счёт', 'погашение']),\n",
       " (['либо', 'зачесть'], 'счёт', ['погашение', 'кредит']),\n",
       " (['зачесть', 'счёт'], 'погашение', ['кредит', 'PAD']),\n",
       " (['счёт', 'погашение'], 'кредит', ['PAD', 'PAD'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected\n",
    "\n",
    "```python\n",
    "[(['PAD', 'PAD'], 'вопрос', ['почему', 'например']),\n",
    " (['PAD', 'вопрос'], 'почему', ['например', 'китайский']),\n",
    " (['вопрос', 'почему'], 'например', ['китайский', 'японский']),\n",
    " (['почему', 'например'], 'китайский', ['японский', 'UNK']),\n",
    " (['например', 'китайский'], 'японский', ['UNK', 'PAD']),\n",
    " (['китайский', 'японский'], 'UNK', ['PAD', 'PAD'])]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['PAD', 'PAD', 'PAD'], 'вернуть', ['денежный', 'средство', 'лицевой']),\n",
       " (['PAD', 'PAD', 'вернуть'], 'денежный', ['средство', 'лицевой', 'счёт']),\n",
       " (['PAD', 'вернуть', 'денежный'], 'средство', ['лицевой', 'счёт', 'либо']),\n",
       " (['вернуть', 'денежный', 'средство'], 'лицевой', ['счёт', 'либо', 'зачесть']),\n",
       " (['денежный', 'средство', 'лицевой'], 'счёт', ['либо', 'зачесть', 'счёт']),\n",
       " (['средство', 'лицевой', 'счёт'], 'либо', ['зачесть', 'счёт', 'погашение']),\n",
       " (['лицевой', 'счёт', 'либо'], 'зачесть', ['счёт', 'погашение', 'кредит']),\n",
       " (['счёт', 'либо', 'зачесть'], 'счёт', ['погашение', 'кредит', 'PAD']),\n",
       " (['либо', 'зачесть', 'счёт'], 'погашение', ['кредит', 'PAD', 'PAD']),\n",
       " (['зачесть', 'счёт', 'погашение'], 'кредит', ['PAD', 'PAD', 'PAD'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_split(sample_text, window=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected\n",
    "\n",
    "```python\n",
    "[(['PAD', 'PAD', 'PAD'], 'вопрос', ['почему', 'например', 'китайский']),\n",
    " (['PAD', 'PAD', 'вопрос'], 'почему', ['например', 'китайский', 'японский']),\n",
    " (['PAD', 'вопрос', 'почему'], 'например', ['китайский', 'японский', 'UNK']),\n",
    " (['вопрос', 'почему', 'например'], 'китайский', ['японский', 'UNK', 'PAD']),\n",
    " (['почему', 'например', 'китайский'], 'японский', ['UNK', 'PAD', 'PAD']),\n",
    " (['например', 'китайский', 'японский'], 'UNK', ['PAD', 'PAD', 'PAD'])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вернуть денежный средство лицевой счёт либо зачесть счёт погашение кредит'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализуйте разделение предложения на примеры методом Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_split(tokens, window):\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    vocab = set(tokens)\n",
    "    \n",
    "    for i in range(0, len(tokens)):\n",
    "        target = tokens[i]\n",
    "        if i == 0:\n",
    "            pass\n",
    "        if i < window:\n",
    "            for c in reversed(range(1, window+1)):\n",
    "                if tokens.index(tokens[i - c]) >= window:\n",
    "                    pass\n",
    "                else:\n",
    "                    splits.append((tokens[i - c], target))\n",
    "            for c in range(1, window+1):\n",
    "                try:\n",
    "                    splits.append((tokens[i + c], target))\n",
    "                except IndexError:\n",
    "                    pass\n",
    "        else:\n",
    "            for c in reversed(range(1, window+1)):\n",
    "                splits.append((tokens[i - c], target))\n",
    "            for c in range(1, window+1):\n",
    "                try:\n",
    "                    splits.append((tokens[i + c], target))\n",
    "                except IndexError:\n",
    "                    pass            \n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = skipgram_split(sample_text, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст: денежный\n",
      "Центральное слово: вернуть\n",
      "\n",
      "Контекст: средство\n",
      "Центральное слово: вернуть\n",
      "\n",
      "Контекст: вернуть\n",
      "Центральное слово: денежный\n",
      "\n",
      "Контекст: средство\n",
      "Центральное слово: денежный\n",
      "\n",
      "Контекст: лицевой\n",
      "Центральное слово: денежный\n",
      "\n",
      "Контекст: вернуть\n",
      "Центральное слово: средство\n",
      "\n",
      "Контекст: денежный\n",
      "Центральное слово: средство\n",
      "\n",
      "Контекст: лицевой\n",
      "Центральное слово: средство\n",
      "\n",
      "Контекст: счёт\n",
      "Центральное слово: средство\n",
      "\n",
      "Контекст: денежный\n",
      "Центральное слово: лицевой\n",
      "\n",
      "Контекст: средство\n",
      "Центральное слово: лицевой\n",
      "\n",
      "Контекст: счёт\n",
      "Центральное слово: лицевой\n",
      "\n",
      "Контекст: либо\n",
      "Центральное слово: лицевой\n",
      "\n",
      "Контекст: средство\n",
      "Центральное слово: счёт\n",
      "\n",
      "Контекст: лицевой\n",
      "Центральное слово: счёт\n",
      "\n",
      "Контекст: либо\n",
      "Центральное слово: счёт\n",
      "\n",
      "Контекст: зачесть\n",
      "Центральное слово: счёт\n",
      "\n",
      "Контекст: лицевой\n",
      "Центральное слово: либо\n",
      "\n",
      "Контекст: счёт\n",
      "Центральное слово: либо\n",
      "\n",
      "Контекст: зачесть\n",
      "Центральное слово: либо\n",
      "\n",
      "Контекст: счёт\n",
      "Центральное слово: либо\n",
      "\n",
      "Контекст: счёт\n",
      "Центральное слово: зачесть\n",
      "\n",
      "Контекст: либо\n",
      "Центральное слово: зачесть\n",
      "\n",
      "Контекст: счёт\n",
      "Центральное слово: зачесть\n",
      "\n",
      "Контекст: погашение\n",
      "Центральное слово: зачесть\n",
      "\n",
      "Контекст: либо\n",
      "Центральное слово: счёт\n",
      "\n",
      "Контекст: зачесть\n",
      "Центральное слово: счёт\n",
      "\n",
      "Контекст: погашение\n",
      "Центральное слово: счёт\n",
      "\n",
      "Контекст: кредит\n",
      "Центральное слово: счёт\n",
      "\n",
      "Контекст: зачесть\n",
      "Центральное слово: погашение\n",
      "\n",
      "Контекст: счёт\n",
      "Центральное слово: погашение\n",
      "\n",
      "Контекст: кредит\n",
      "Центральное слово: погашение\n",
      "\n",
      "Контекст: счёт\n",
      "Центральное слово: кредит\n",
      "\n",
      "Контекст: погашение\n",
      "Центральное слово: кредит\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in splits:\n",
    "    print('Контекст:', sample[0])\n",
    "    print('Центральное слово:', sample[1], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('денежный', 'вернуть'),\n",
       " ('средство', 'вернуть'),\n",
       " ('вернуть', 'денежный'),\n",
       " ('средство', 'денежный'),\n",
       " ('лицевой', 'денежный'),\n",
       " ('вернуть', 'средство'),\n",
       " ('денежный', 'средство'),\n",
       " ('лицевой', 'средство'),\n",
       " ('счёт', 'средство'),\n",
       " ('денежный', 'лицевой'),\n",
       " ('средство', 'лицевой'),\n",
       " ('счёт', 'лицевой'),\n",
       " ('либо', 'лицевой'),\n",
       " ('средство', 'счёт'),\n",
       " ('лицевой', 'счёт'),\n",
       " ('либо', 'счёт'),\n",
       " ('зачесть', 'счёт'),\n",
       " ('лицевой', 'либо'),\n",
       " ('счёт', 'либо'),\n",
       " ('зачесть', 'либо'),\n",
       " ('счёт', 'либо'),\n",
       " ('счёт', 'зачесть'),\n",
       " ('либо', 'зачесть'),\n",
       " ('счёт', 'зачесть'),\n",
       " ('погашение', 'зачесть'),\n",
       " ('либо', 'счёт'),\n",
       " ('зачесть', 'счёт'),\n",
       " ('погашение', 'счёт'),\n",
       " ('кредит', 'счёт'),\n",
       " ('зачесть', 'погашение'),\n",
       " ('счёт', 'погашение'),\n",
       " ('кредит', 'погашение'),\n",
       " ('счёт', 'кредит'),\n",
       " ('погашение', 'кредит')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_split(sample_text, window=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected\n",
    "\n",
    "```python\n",
    "[('почему', 'вопрос'),\n",
    " ('например', 'вопрос'),\n",
    " ('вопрос', 'почему'),\n",
    " ('например', 'почему'),\n",
    " ('китайский', 'почему'),\n",
    " ('вопрос', 'например'),\n",
    " ('почему', 'например'),\n",
    " ('китайский', 'например'),\n",
    " ('японский', 'например'),\n",
    " ('почему', 'китайский'),\n",
    " ('например', 'китайский'),\n",
    " ('японский', 'китайский'),\n",
    " ('UNK', 'китайский'),\n",
    " ('например', 'японский'),\n",
    " ('китайский', 'японский'),\n",
    " ('UNK', 'японский'),\n",
    " ('китайский', 'UNK'),\n",
    " ('японский', 'UNK')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('денежный', 'вернуть'),\n",
       " ('средство', 'вернуть'),\n",
       " ('лицевой', 'вернуть'),\n",
       " ('вернуть', 'денежный'),\n",
       " ('средство', 'денежный'),\n",
       " ('лицевой', 'денежный'),\n",
       " ('счёт', 'денежный'),\n",
       " ('вернуть', 'средство'),\n",
       " ('денежный', 'средство'),\n",
       " ('лицевой', 'средство'),\n",
       " ('счёт', 'средство'),\n",
       " ('либо', 'средство'),\n",
       " ('вернуть', 'лицевой'),\n",
       " ('денежный', 'лицевой'),\n",
       " ('средство', 'лицевой'),\n",
       " ('счёт', 'лицевой'),\n",
       " ('либо', 'лицевой'),\n",
       " ('зачесть', 'лицевой'),\n",
       " ('денежный', 'счёт'),\n",
       " ('средство', 'счёт'),\n",
       " ('лицевой', 'счёт'),\n",
       " ('либо', 'счёт'),\n",
       " ('зачесть', 'счёт'),\n",
       " ('счёт', 'счёт'),\n",
       " ('средство', 'либо'),\n",
       " ('лицевой', 'либо'),\n",
       " ('счёт', 'либо'),\n",
       " ('зачесть', 'либо'),\n",
       " ('счёт', 'либо'),\n",
       " ('погашение', 'либо'),\n",
       " ('лицевой', 'зачесть'),\n",
       " ('счёт', 'зачесть'),\n",
       " ('либо', 'зачесть'),\n",
       " ('счёт', 'зачесть'),\n",
       " ('погашение', 'зачесть'),\n",
       " ('кредит', 'зачесть'),\n",
       " ('счёт', 'счёт'),\n",
       " ('либо', 'счёт'),\n",
       " ('зачесть', 'счёт'),\n",
       " ('погашение', 'счёт'),\n",
       " ('кредит', 'счёт'),\n",
       " ('либо', 'погашение'),\n",
       " ('зачесть', 'погашение'),\n",
       " ('счёт', 'погашение'),\n",
       " ('кредит', 'погашение'),\n",
       " ('зачесть', 'кредит'),\n",
       " ('счёт', 'кредит'),\n",
       " ('погашение', 'кредит')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_split(sample_text, window=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected\n",
    "\n",
    "```python\n",
    "[('почему', 'вопрос'),\n",
    " ('например', 'вопрос'),\n",
    " ('китайский', 'вопрос'),\n",
    " ('вопрос', 'почему'),\n",
    " ('например', 'почему'),\n",
    " ('китайский', 'почему'),\n",
    " ('японский', 'почему'),\n",
    " ('вопрос', 'например'),\n",
    " ('почему', 'например'),\n",
    " ('китайский', 'например'),\n",
    " ('японский', 'например'),\n",
    " ('UNK', 'например'),\n",
    " ('вопрос', 'китайский'),\n",
    " ('почему', 'китайский'),\n",
    " ('например', 'китайский'),\n",
    " ('японский', 'китайский'),\n",
    " ('UNK', 'китайский'),\n",
    " ('почему', 'японский'),\n",
    " ('например', 'японский'),\n",
    " ('китайский', 'японский'),\n",
    " ('UNK', 'японский'),\n",
    " ('например', 'UNK'),\n",
    " ('китайский', 'UNK'),\n",
    " ('японский', 'UNK')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "\n",
    "for text in corpus:\n",
    "    for token in text:\n",
    "        if token not in word2index:\n",
    "            word2index[token] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13076"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1282, 8436, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word2index[tok] if tok in word2index else word2index['UNK'] for tok in 'мама мыть рама'.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Dataset\n",
    "В торче есть очень удобная читалка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# игрушечный датасет\n",
    "# 121535 примера, 4 фичи, 3 класса\n",
    "some_data_x = np.random.rand(121535, 4)\n",
    "some_data_y = np.random.randint(3, size=(121535,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21336309, 0.07833123, 0.05747241, 0.12036598],\n",
       "       [0.97712413, 0.94836216, 0.39159031, 0.65874754],\n",
       "       [0.25876942, 0.00825557, 0.83715008, 0.3990048 ],\n",
       "       [0.38362676, 0.63744464, 0.77513582, 0.0462719 ],\n",
       "       [0.48612308, 0.81620841, 0.81014846, 0.75167817],\n",
       "       [0.6278356 , 0.32942061, 0.28632835, 0.08923994],\n",
       "       [0.71136491, 0.8107372 , 0.25042029, 0.9208095 ],\n",
       "       [0.84227797, 0.97717574, 0.87010521, 0.39954141],\n",
       "       [0.39489049, 0.61894733, 0.27673239, 0.27452323],\n",
       "       [0.99766391, 0.72883587, 0.00878005, 0.85856438]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# соверешенно игрушечный, просто цифры\n",
    "some_data_x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_x, data_y):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        # Нужно обязательно определить эту функцию\n",
    "        # Должна возвращать размер датасета\n",
    "        \n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Еще нужно определить этот метод\n",
    "        # То есть как мы будем доставать наши данные по индексу\n",
    "        \n",
    "        return self.data_x[idx], self.data_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataset = ToyDataset(some_data_x, some_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.6278356 , 0.32942061, 0.28632835, 0.08923994]), 1),\n",
       " (array([0.97221751, 0.03151782, 0.65527603, 0.21730833]), 0))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_dataset[5], some_dataset[467]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_loader = DataLoader(some_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, tensor([[0.1118, 0.3001, 0.7603, 0.2554],\n",
       "         [0.2645, 0.4002, 0.2909, 0.4311],\n",
       "         [0.4763, 0.1332, 0.1712, 0.9265],\n",
       "         [0.1039, 0.4350, 0.7044, 0.3552],\n",
       "         [0.7653, 0.9810, 0.0284, 0.5955],\n",
       "         [0.3463, 0.5253, 0.8671, 0.2731],\n",
       "         [0.9278, 0.6161, 0.5108, 0.5480],\n",
       "         [0.8254, 0.8165, 0.9830, 0.2257],\n",
       "         [0.6187, 0.3340, 0.6548, 0.5575],\n",
       "         [0.3797, 0.6396, 0.8053, 0.0404],\n",
       "         [0.3299, 0.3154, 0.0845, 0.6925],\n",
       "         [0.5895, 0.0879, 0.4362, 0.8049],\n",
       "         [0.6417, 0.1567, 0.2560, 0.7827],\n",
       "         [0.5861, 0.7368, 0.3761, 0.7101],\n",
       "         [0.7479, 0.6936, 0.8469, 0.6032],\n",
       "         [0.3045, 0.3536, 0.7151, 0.3775]], dtype=torch.float64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x, y in some_loader:\n",
    "    break\n",
    "    \n",
    "len(x), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x, y in some_loader:\n",
    "    pass\n",
    "\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# почему 13?\n",
    "# потому что количество наших данных нацело не делится на 16\n",
    "# и поэтому последний батч меньше 16-ти\n",
    "len(some_dataset) % 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# А зачем?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_x, data_y):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        # Нужно обязательно определить эту функцию\n",
    "        # Должна возвращать размер датасета\n",
    "        \n",
    "        return len(self.data_x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_pow_features(x, n=2):\n",
    "        \n",
    "        return np.concatenate([x, x ** n]) \n",
    "    \n",
    "    @staticmethod\n",
    "    def add_log_features(x):\n",
    "        \n",
    "        return np.concatenate([x, np.log(x)]) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Еще нужно определить этот метод\n",
    "        # То есть как мы будем доставать наши данные по индексу\n",
    "        \n",
    "        x = self.data_x[idx]\n",
    "        \n",
    "        # внутри датасета мы можем делать все что угодно с нашими данными\n",
    "        # например выше определим функции, которые добавляют степенные фичи\n",
    "        x = self.add_pow_features(x, n=2)\n",
    "        x = self.add_pow_features(x, n=3)\n",
    "        # и еще возьмем логарифмические фичи\n",
    "        x = self.add_log_features(x)\n",
    "        \n",
    "        y = self.data_y[idx]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataset = ToyDataset(some_data_x, some_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_loader = DataLoader(dataset=toy_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in toy_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 32])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1336e-01,  7.8331e-02,  5.7472e-02,  ..., -1.5281e+01,\n",
       "         -1.7139e+01, -1.2703e+01],\n",
       "        [ 9.7712e-01,  9.4836e-01,  3.9159e-01,  ..., -3.1811e-01,\n",
       "         -5.6252e+00, -2.5045e+00],\n",
       "        [ 2.5877e-01,  8.2556e-03,  8.3715e-01,  ..., -2.8781e+01,\n",
       "         -1.0665e+00, -5.5127e+00],\n",
       "        ...,\n",
       "        [ 1.3930e-01,  9.4364e-01,  8.5012e-01,  ..., -3.4807e-01,\n",
       "         -9.7425e-01, -2.1640e+00],\n",
       "        [ 7.0188e-01,  4.1989e-01,  3.8678e-01,  ..., -5.2065e+00,\n",
       "         -5.6994e+00, -1.0717e+01],\n",
       "        [ 4.4978e-01,  3.8498e-01,  9.8764e-01,  ..., -5.7274e+00,\n",
       "         -7.4592e-02, -5.9350e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# заметим, что мы сразу получаем торчовый формат данных\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 0, 1, 2, 0, 2, 1,\n",
       "        2, 1, 2, 2, 0, 1, 2, 1, 0, 1, 2, 0, 2, 1, 0, 0, 2, 0, 1, 2, 0, 2, 0, 2,\n",
       "        2, 2, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 0, 1, 1,\n",
       "        1, 2, 1, 0, 1, 1, 2, 1, 2, 0, 2, 1, 1, 0, 1, 0, 2, 0, 0, 2, 0, 1, 0, 1,\n",
       "        0, 2, 2, 0, 1, 2, 2, 0, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 0, 0, 1, 2,\n",
       "        1, 0, 2, 1, 2, 0, 1, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Если вы ничего здесь не понимаете, то вернитесь в конец первой домашки, там все объясняется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(32, 16),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(16, 8),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(8, 3))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0962421894073486"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    prediction = model(x.float())\n",
    "\n",
    "    loss = criterion(prediction, y)\n",
    "    \n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевые датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 word2index,\n",
    "                 window=2,\n",
    "                 unk_token='UNK',\n",
    "                 pad_token='PAD',\n",
    "                 collect_verbose=True):\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.word2index = word2index\n",
    "        self.index2word = {value: key for key, value in self.word2index.items()}\n",
    "        self.window = window\n",
    "\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = self.word2index[self.unk_token]\n",
    "\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_index = len(self.word2index)\n",
    "\n",
    "        self.collect_verbose = collect_verbose\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        self.collect_data()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def _split_function(self, tokenized_text):\n",
    "\n",
    "        splits = []\n",
    "\n",
    "        for n in range(len(tokenized_text)):\n",
    "            left_context = tokenized_text[np.maximum(n - self.window, 0):n]\n",
    "            left_context = ([self.pad_index] * (self.window - len(left_context))) + left_context\n",
    "\n",
    "            central_word = tokenized_text[n]\n",
    "\n",
    "            right_context = tokenized_text[n + 1:n + self.window + 1]\n",
    "            right_context = right_context + ([self.pad_index] * (self.window - len(right_context)))\n",
    "\n",
    "            splits.append((left_context + right_context, central_word))\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def indexing(self, tokenized_text):\n",
    "\n",
    "        return [self.word2index[token] if token in self.word2index else self.unk_index for token in tokenized_text]\n",
    "\n",
    "    def collect_data(self):\n",
    "\n",
    "        corpus = tqdm(self.corpus, disable=not self.collect_verbose)\n",
    "\n",
    "        for tokenized_text in corpus:\n",
    "            indexed_text = self.indexing(tokenized_text)\n",
    "            cbow_examples = self._split_function(indexed_text)\n",
    "\n",
    "            self.data.extend(cbow_examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        context, central_word = self.data[idx]\n",
    "\n",
    "        context = torch.Tensor(context)\n",
    "\n",
    "        return context, central_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мы будем учить модель Skipgram\n",
    "Реализуйте читалку данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 word2index,\n",
    "                 window=2,\n",
    "                 unk_token='UNK',\n",
    "                 collect_verbose=True):\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.word2index = word2index\n",
    "        self.index2word = {value: key for key, value in self.word2index.items()}\n",
    "        self.window = window\n",
    "\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = self.word2index[self.unk_token]\n",
    "\n",
    "        self.collect_verbose = collect_verbose\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        self.collect_data()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def _split_function(self, tokenized_text):\n",
    "\n",
    "        splits = []\n",
    "        window = 2\n",
    "        \n",
    "        tokens = tokenized_text\n",
    "        vocab = set(tokens)\n",
    "\n",
    "        for i in range(0, len(tokens)):\n",
    "            target = tokens[i]\n",
    "            if i == 0:\n",
    "                pass\n",
    "            if i < window:\n",
    "                for c in reversed(range(1, window+1)):\n",
    "                    if IndexError:\n",
    "                        pass\n",
    "                    else:\n",
    "                        splits.append((tokens[i - c], target))\n",
    "                for c in range(1, window+1):\n",
    "                    try:\n",
    "                        splits.append((tokens[i + c], target))\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "            else:\n",
    "                for c in reversed(range(1, window+1)):\n",
    "                    splits.append((tokens[i - c], target))\n",
    "                for c in range(1, window+1):\n",
    "                    try:\n",
    "                        splits.append((tokens[i + c], target))\n",
    "                    except IndexError:\n",
    "                        pass            \n",
    "\n",
    "        return splits\n",
    "\n",
    "    def indexing(self, tokenized_text):\n",
    "\n",
    "        return [self.word2index[token] if token in self.word2index else self.unk_index for token in tokenized_text]\n",
    "\n",
    "    def collect_data(self):\n",
    "\n",
    "        corpus = tqdm(self.corpus, disable=not self.collect_verbose)\n",
    "\n",
    "        for tokenized_text in corpus:\n",
    "            indexed_text = self.indexing(tokenized_text)\n",
    "            skipgram_examples = self._split_function(indexed_text)\n",
    "\n",
    "            self.data.extend(skipgram_examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        context, central_word = self.data[idx]\n",
    "\n",
    "        return context, central_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можете положить SkipgramDataset в отдельный файлик, например word2vec_utils и относительным импортом достать его \n",
    "#from .word2vec_utils import SkipgramDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 2611/100000 [00:00<00:03, 26106.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 5357/100000 [00:00<00:03, 26498.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 7933/100000 [00:00<00:03, 26272.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█▏        | 11259/100000 [00:00<00:03, 28024.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 14616/100000 [00:00<00:02, 29485.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 18136/100000 [00:00<00:02, 30994.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 20980/100000 [00:00<00:02, 27310.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 24174/100000 [00:00<00:02, 28550.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 27197/100000 [00:00<00:02, 29034.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 30065/100000 [00:01<00:02, 26946.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 33013/100000 [00:01<00:02, 27658.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 35787/100000 [00:01<00:02, 27521.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▊      | 38545/100000 [00:01<00:02, 25055.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 41260/100000 [00:01<00:02, 25647.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 43861/100000 [00:01<00:02, 24501.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 46664/100000 [00:01<00:02, 25462.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████▉     | 49621/100000 [00:01<00:01, 26568.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 52724/100000 [00:01<00:01, 27765.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 55772/100000 [00:02<00:01, 28527.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▊    | 58738/100000 [00:02<00:01, 28852.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 61743/100000 [00:02<00:01, 29197.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▍   | 64681/100000 [00:02<00:01, 28109.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 67762/100000 [00:02<00:01, 28868.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 70669/100000 [00:02<00:01, 27890.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 73864/100000 [00:02<00:00, 28994.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 76924/100000 [00:02<00:00, 29457.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████▉  | 79918/100000 [00:02<00:00, 29598.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 82893/100000 [00:02<00:00, 28711.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 85984/100000 [00:03<00:00, 29337.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 89055/100000 [00:03<00:00, 29735.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 92060/100000 [00:03<00:00, 29827.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 95051/100000 [00:03<00:00, 29545.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 28313.93it/s][A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "dataset = SkipgramDataset(corpus, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  32,  264, 6810, 3925,    1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 406,  512, 2725, 7163, 9530])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512]), torch.Size([512]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, pad_index):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if pad_index > 0:\n",
    "            vocab_size += 1\n",
    "        \n",
    "        self.in_embedding = torch.nn.Embedding(num_embeddings=vocab_size, \n",
    "                                               embedding_dim=embedding_dim,\n",
    "                                               padding_idx=pad_index)\n",
    "        \n",
    "        self.out_embedding = torch.nn.Linear(in_features=embedding_dim,\n",
    "                                             out_features=vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.in_embedding(x).sum(dim=-2)\n",
    "        x = self.out_embedding(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мы будем учить модель Skipgram\n",
    "Реализуйте ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n",
    "class SkipGram(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_embedding = torch.nn.Embedding(num_embeddings=vocab_size, \n",
    "                                               embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.out_embedding = torch.nn.Linear(in_features=embedding_dim,\n",
    "                                             out_features=vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.in_embedding(x)\n",
    "        x = self.out_embedding(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from word2vec_utils import SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# размерность эмбеддинга\n",
    "# маленькая, чтобы мы могли недолго поучить ворд2век и увидеть результаты\n",
    "EMBEDDING_DIM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(vocab_size=len(word2index), embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 13076])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "# aka loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишите обучалку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1:   0%|          | 0/2883126 [21:11<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Epoch 1:   0%|          | 0/2883126 [19:09<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [18:44<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [17:59<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [15:37<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [14:57<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [13:56<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [11:12<?, ?it/s]\n",
      "Epoch 1:   0%|          | 512/2883126 [09:20<876:43:14,  1.09s/it, loss=76.1]\n",
      "Epoch 1:   0%|          | 0/2883126 [08:27<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [07:53<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [06:49<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/2883126 [00:31<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "losses = []\n",
    "\n",
    "for n_epoch in range(epochs):\n",
    "\n",
    "    try:\n",
    "\n",
    "        progress_bar = tqdm(total=len(dataset_loader.dataset), desc='Epoch {}'.format(n_epoch + 1))\n",
    "\n",
    "        for x, y in dataset_loader:\n",
    "\n",
    "            inputs, targets = zip(*dataset_loader)\n",
    "        \n",
    "            inputs = torch.cat(inputs) # B x 1\n",
    "            targets = torch.cat(targets) # B x 1\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss = model(inputs)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.data.tolist())\n",
    "            \n",
    "            progress_bar.set_postfix(loss=np.mean(losses[-100:]))\n",
    "\n",
    "            progress_bar.update(x.shape[0])\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        progress_bar.close()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "losses = []\n",
    "\n",
    "for n_epoch in range(epochs):\n",
    "\n",
    "    try:\n",
    "\n",
    "        progress_bar = tqdm(total=len(dataset_loader.dataset), desc='Epoch {}'.format(n_epoch + 1))\n",
    "        torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('SkipGram Training Process')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка, что хоть что-то выучилось\n",
    "assert np.mean(losses[-1000:]) < 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.in_embedding.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(embedding_matrix, token2id, word1, word2):\n",
    "    \n",
    "    i1 = token2id[word1]\n",
    "    i2 = token2id[word2]\n",
    "    \n",
    "    v1, v2 = embedding_matrix[i1], embedding_matrix[i2]\n",
    "    \n",
    "    v1_n = v1.div(v1.norm(keepdim=True))\n",
    "    v2_n = v2.div(v2.norm(keepdim=True))\n",
    "    \n",
    "    similarity = torch.dot(v1_n, v2_n).item()\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Косинусная близость\n",
    "От 0 до 1, где 0 - вектора абсолютно разные, где 1 - идентичные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(embedding_matrix, word2index, 'день', 'месяц')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(embedding_matrix, word2index, 'минута', 'месяц')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(embedding_matrix, word2index, 'сотрудник', 'сотрудница')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(embedding_matrix, word2index, 'вклад', 'перевод')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_word = random.choice(list(word2index.keys()))\n",
    "sim = cos_sim(embedding_matrix, word2index, 'день', random_word)\n",
    "'Косинусная близость слова \"день\" к случайному выбраному слову \"{}\" равна {:.3f}'.format(random_word, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "\n",
    "for text in corpus:\n",
    "    for token in text:\n",
    "        if token in freq:\n",
    "            freq[token] += 1\n",
    "        else:\n",
    "            freq[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq = [(k, freq[k]) for k in sorted(freq, key=freq.get, reverse=True)]\n",
    "top_sorted_freq = sorted_freq[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca', random_state=42, verbose=2)\n",
    "reduced = tsne.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = [a for a,_ in top_sorted_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = [word2index[word] for word in top_words]\n",
    "x_coords = [coords[0] for coords in reduced[inds]]\n",
    "y_coords = [coords[1] for coords in reduced[inds]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y, word) in zip(x_coords, y_coords, top_words):\n",
    "    plt.scatter(x, y, marker='.', color='blue')\n",
    "    plt.text(x+0.01, y+0.01, word, fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка\n",
    "1. Вы добрались сюда и все работает, значит уже получили 7 баллов.\n",
    "2. 8 баллов - Взяли корпус для оценка качества эмбеддингов [здесь](https://rusvectores.org/static/testsets/ru_simlex965_tagged.tsv). Описание к нему [здесь](https://arxiv.org/pdf/1801.06407.pdf). Его английская версия для понимания, того что же это такое [тут](https://fh295.github.io/simlex.html). Если в кратце - он похож а гугл аналогии, просто иначе составлен. Определили качество своих эмбеддингов. Как качество измерить? Можете все значения отнормировать (привести к 1) и затем считать MSE между тем что у вас и что в оригинале.\n",
    "3. 9 баллов - Поставили эксперименты, поменяли любые параметры, хоть корпус увеличили или как то почистили. Показали метрики до и после. После должно быть лучше, иначе это все еще 8 баллов.\n",
    "4. 10 баллов - удивили своим подходом (или просто удивили) пока делили на 9 баллов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
